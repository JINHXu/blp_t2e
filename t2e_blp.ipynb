{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Read in and pre-process data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download data\ndownload.file(\"http://crr.ugent.be/blp/txt/blp-items.txt.zip\", \"blp.zip\") \n# Open connection to .txt file in .zip archive\nconn = unz(\"blp.zip\",\"blp-items.txt\") \n# Read data\nblp = read.table(conn, head=TRUE)\n# Remove words without RT\nblp = blp[-which(is.na(blp$rt)),]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspect data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of rows(words) and columns\ndim(blp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average response time\nmean(blp$rt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the average response time for the word \"hear\"\nblp$rt[which(blp$spelling == 'hear')]\n# \"dear\"\nblp$rt[which(blp$spelling == 'dear')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the word with the longest average response time\nblp$spelling[which(blp$rt == max(blp$rt))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the distribution of the response times in the BLP. \nplot(density(blp$rt), xlab = 'response time(ms)', ylab = 'density')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add 'status' colunm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a column to the data that contains the event status for all words.\nblp$status = 1\n# Create temporary data frame\nblp_tmp = blp\n# Update response times\nblp_tmp$rt[which(blp_tmp$rt>1000)] = 1000\nblp_tmp$status[which(blp_tmp$rt == 1000)] = 0\n# inspect\nblp_tmp$status[which(blp_tmp$rt == 1000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many words have an event status of 0\nlength(blp_tmp$status[which(blp_tmp$rt == 1000)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What percentage of the data do these words represent?\nlength(blp_tmp$status[which(blp_tmp$rt == 1000)])/length(blp$rt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Objective functions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## PDF\n\n# Generate the probability density function for the response times in the BLP\nd = density(blp$rt, from = 300, to = 1650, n = 1351)\n# What is the probability density estimate for t = 600? \nd$y[which(d$x == 600)]\n# What is the probability density estimate for t = 900? \nd$y[which(d$x == 900)]\n# What is the relative likelihood of a response time of 600 ms as compared to a response time of 900 ms?\nd$y[which(d$x == 600)]/d$y[which(d$x == 900)]\n# At what point in time does the probability density function reach its maximum?\nd$x[which(d$y==max(d$y))]\n# Plot the probability density function for the response times in the BLP. \nplot(d)\n# How does the probability density function for the lexical decision latencies in the BLP compare to the probability density function for the naming latencies in the ELP?\nhead(blp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## CDF\n\n# Generate the cumulative density function for the response times in the BLP.\ncdf = ecdf(blp$rt)\n\n# Access the environment of cdf\ncdf = environment(cdf)\n\n# What proportion of words has been responded to at 600ms after stimulus onset? \ncdf$y[which(cdf$x==600)]\n# How about at 900 ms after stimulus onset?\ncdf$y[which(cdf$x==900)]\n# Plot the cumulative density function for the response times in the BLP.\nplot(ecdf(blp$rt), xlab = \"response time (ms)\", ylab = \"F(x)\", main = \"\",\n     xlim = c(0,3000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SURVIVAL\n\n\n# Load survival library\nlibrary(survival)\n\n# All words in the BLP were responded to. Create a column status that is set to 1 for all words.\nblp$status = 1\n\n# Create a survival object that contains the time event tuple for all words.\nsurv = Surv(time = blp$rt, event = blp$status)\n\n# Round the response latencies(rt) in the BLP to the nearest integer using the round() function. \nblp$rt = round(blp$rt)\n\n# Next, generate the Kaplan-Meier estimate of the survival function for the lexical decision latencies(rt) in the BLP.\nkm_estimator = survfit(surv~1)\n\n# Plot the estimated survival curve.\nplot(km_estimator, xlab = \"rounded rt (ms)\", ylab = \"S(t)\", conf.int = FALSE)\n\n# What proportion of words has not been responded to at 600 ms after stimulus onset? \nkm_estimator$surv[which(km_estimator$time == 600)]\n# How about at 900 ms after stimulus onset? \nkm_estimator$surv[which(km_estimator$time == 900)]\n# What is the relation between the survival function and the cumulative density function?\n\n# a simple illustration\nhold = TRUE\n\nfor (rt in blp$rt)\n    {\n    if (km_estimator$surv[which(km_estimator$time == rt)] + cdf$y[which(cdf$x==rt)] != 1)\n        {print('S(t) = 1 − F(t) does not hold???because of rounded???')\n         hold = FALSE\n        break}\n}\n\nif (hold){print('S(t) = 1 − F(t)')}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate the hazard function for the response times in the BLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hazard\n\n\n# Restrict density function time points\nd$y = d$y[which(d$x%in%km_estimator$time)]\nd$x = d$x[which(d$x%in%km_estimator$time)]\n# Define hazard function\nhazard = d$y / km_estimator$surv\n\n# Hazard plot\nplot(km_estimator$time, hazard, ylim = c(0, 0.1), xlab = \"rounded response time (ms)\",\n     ylab = expression(lambda(t)),type=\"l\")\n\n# Cumulative hazard function plot\nplot(km_estimator, cumhaz = TRUE, conf.int = FALSE, xlab = \"rounded response time (ms)\",\n     ylab = expression(Lambda(t)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorical predictors**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load survival library\nlibrary(survival)\n\n# Round response times to integers\nblp$rt = round(blp$rt)\n\n# How many stimuli are present in the data?\nnrow(blp)\n\n# Visualize the number of words and nonwords in the data with a barplot.\ntab = table(blp$lexicality)\ntab\n\noptions(repr.plot.width=5, repr.plot.height=5)\nbarplot(tab, xlab = \"lexicality\", ylab = \"number of stimuli\", ylim = c(0, 30000))\n\n# Add a column with the event status to the data frame. Set the event status to 1 for each trial. \nblp$status = 1\n\n# Next, create a survival object with the time-event tuple for the lexical decision data.\nsurv = Surv(time = blp$rt, event = blp$status)\n\n# Fit survival curves to the data with the survfit() function. \nkm_estimator = survfit(surv~1)\n\n# What are the median response times for words and for nonwords?\nmedian(blp$rt[which(blp$lexical == 'W')])\nmedian(blp$rt[which(blp$lexical == 'N')])\n\n# alternative solution\n\nsurvival = survfit(surv ~ lexicality, data = blp)\nsurvival\n\n# Download survival package\ndevtools::install_github(\"kassambara/survminer\", build_vignettes = FALSE, upgrade = \"never\", lib=\"/kaggle/working\")\n\n\n# Load survival library\nlibrary(survminer)\n\n# Plot the probability density function for words\nplot(density(blp$rt[which(blp$lexicality == \"W\")]), xlab = \"time (ms)\",\n     ylab = \"pdf\", main = \"\", ylim = c(0, 0.008))\n# Add probability density function for nonwords\nlines(density(blp$rt[which(blp$lexicality == \"N\")]), col = \"blue\")\n\n# survival plot\noptions(repr.plot.width=10, repr.plot.height=5)\nplot(survival, xlab = \"time (ms)\", ylab = \"S(t)\", col = c(\"black\", \"blue\"))\n\n# log-rank test\nsurv.test = survdiff(surv ~ lexicality, data = blp)\nsurv.test\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download data\ndownload.file(\"http://crr.ugent.be/blp/txt/blp-stimuli.txt.zip\",\n              \"blp.stimuli.zip\")\n# Open connection to .txt file in .zip archive\nconn = unz(\"blp.stimuli.zip\",\"blp-stimuli.txt\")\n# Read data\nblp.stimuli = read.table(conn, head=TRUE, sep=\"\\t\")\n\n# in spect data\nhead(blp.stimuli)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Restrict to the relevant columns\nblp.stimuli = blp.stimuli[,c(\"spelling\", \"synclass\")]\n# Restrict to relevant parts-of-speech categories\nblp.stimuli = blp.stimuli[which(blp.stimuli$synclass%in%\n  c(\"Noun\", \"Verb\", \"Adjective\", \"Adverb\")),]\n# Drop unused levels from the data frame\nblp.stimuli = droplevels(blp.stimuli)\n\n# inspect restricted data\nhead(blp.stimuli)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A barplot to visualize the distribution of parts-of-speech categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntab = table(blp.stimuli$synclass)\n# bar-plot\nbarplot(tab, xlab = \"synclass\", ylab = \"number of trials\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add the lexical information to the data frame with the join() function from the plyr package.\n\n# Download survival package\ninstall.packages(\"plyr\")\n# Load plyr library\nlibrary(plyr)\n\n# use the join() function to add the lexical information to the data frame with the response times\n\n# Add the lexical information\nblp = join(blp, blp.stimuli, type = \"inner\")\n\nhead(blp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit survival curves for the parts-of-speech categories. Inspect the results.\n\n# Load library\nlibrary(survival)\n\n# Generate survival curves\nsurv.cat.pos = survfit(Surv(rt, status) ~ synclass, data = blp)\n\n\nsurv.cat.pos\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value of the survival function for each parts-of-speech category at 600 ms after stimulus onset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load survminer library\nlibrary(survminer)\n# Create summary of survival curve\nsurv.cat.pos.sum = surv_summary(surv.cat.pos)\n\n# Define columns of interest\ncols = c(\"time\",\"surv\",\"upper\",\"lower\",\"synclass\")\n# Inspect survival curves at 600 ms\nsurv.cat.pos.sum[which(surv.cat.pos.sum$time==600), cols]\n# Also retrieve the probability of survival for each parts-of-speech category at 900 ms after stimulus onset. \nsurv.cat.pos.sum[which(surv.cat.pos.sum$time==900), cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the survival curves for the parts-of-speech categories.\nplot(surv.cat.pos, xlab = \"time (ms)\", ylab = \"S(t)\",\n     col = c(\"#00a37a\", \"#000aa3\", \"#a30000\", \"#a300a3\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical test: log-rank test\nsurv.cat.pos.test = survdiff(Surv(rt, status) ~ synclass,\n  data = blp)\n\n# Inspect\nsurv.cat.pos.test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"conlude from the log-rank test inspection: <br>\nThe lower the p−value, the more confidently we can reject the null hypothesis\np-value is significatly small, we can reject the null hypothesis\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical test: post-hoc test for pairwise differences\nsurv.cat.pos.pairwise.test = pairwise_survdiff(Surv(rt, status) ~ synclass,\n  data = blp, p.adjust.method = \"none\")\n\n# Inspect\nsurv.cat.pos.pairwise.test","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":4}